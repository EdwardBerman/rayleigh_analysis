# A systematic investigation of smoothing in GNNs via the Rayleigh Quotient

From yours truly

## Set Up Instructions

### General

1. This repository contains a submodule. To run this repository and access the submodule, run `git clone https://github.com/EdwardBerman/rayleigh_analysis.git` followed by `git submodule update --init --recursive`. The repo will require non Pythonic dependencies, you will need to run `sudo apt install cmake gfortran`.
2. Install poetry and run `poetry install`
3. Install wandb via and login via `wandb login [api key]`

### Simulated Heat diffusion on graphs

1. To generate the heat diffusion data on a graph, do: `python3 -m toy_heat_diffusion.heat_data --n_sources 20 --minheat 1 --maxheat 1  --num_graphs 10000 --size_mean 10 --size_std 2 --time_max 10 --time_step 0.5`
2. To train a GCN on the heat diffusion data on graph, do `python3 -m toy_heat_diffusion.train --data_dir toy_heat_diffusion/data --start_time 0.0 --train_steps 5 --eval_steps 2 --model gcn --layers 12 --hidden 128 --epochs 200 --entity_name [wandb entity name] --project_name [wandb project name]`
3. To train the equivalent(ish) Separable Unitary GCN on the heat diffusion data on graph, do `python3 -m toy_heat_diffusion.train --data_dir toy_heat_diffusion/data --start_time 0.0 --train_steps 5 --eval_steps 2 --model separable_unitary --layers 12 --hidden 128 --epochs 200 --entity_name [wandb entity name] --project_name [wandb project name]`. 
4. For Lie Unitary GCN, simply change the `--model` flag to `lie_unitary` and add the flag `--act Identity`. The bias and skip connections will be set to false by default to keep the model unitary.

### PDEs on Meshes

1. The heat and wave datasets can be generated by running `python3 -m external.custom_hermes.generate_heat` and `python3 -m external.custom_hermes.generate_wave`
2. We generated some heat and wave data with some default settings (i.e. what is in the submodule) and placed it in `data`
3. You can run the training via `python3 -m external.custom_hermes.train dataset=heat backbone=hermes`, this is also in `shell_scripts/mesh.sh`. That being said, we use the default Checkpoints supplied from the original Hermes paper for evaluation. The wandb paths are specified in `external/custom_hermes/conf/train.yaml`
4. Models are evaluated using `shell_scripts/eval_mesh.sh` or `python3 -m external.custom_hermes.eval_rollout dataset=heat backbone=hermes model_save_path=model_checkpoints/[model pt file]`

### Long Range Graph Benchmark

1. The longe range graph benchmark can be downloaded by running `python3 -m data_preprocessing.long_range_graph_benchmark`
2. [Optional] Poke around with the datasets. Run `python3 -m data_preprocessing.homophily` to get the homophily distribution of graphs for the node level classification tasks. 
3. Run peptides with [insert here after done]


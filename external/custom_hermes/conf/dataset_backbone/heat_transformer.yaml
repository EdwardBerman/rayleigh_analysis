# @package _global_

train:
  epochs: 100
  batch_size: 1
  lr: 7e-3

optimizer:
  _target_: torch.optim.Adam
  lr: ${train.lr}
  weight_decay: 1e-5

# params: 40,093
backbone:
  net:
    state_size: 1
    w_size: 256
    n_attention: 2
    nb_gn: 2
    n_heads: 4

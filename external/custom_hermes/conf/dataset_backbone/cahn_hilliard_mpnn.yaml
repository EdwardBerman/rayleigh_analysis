# @package _global_

train:
  epochs: 100
  batch_size: 1
  lr: 7e-4

optimizer:
  _target_: torch.optim.Adam
  lr: ${train.lr}
  weight_decay: 1e-5

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   T_max: ${train.epochs}


backbone:
  net:
    message_dims:
      - - ${dataset.in_dim}
        - 128
        - 128
        - 128
        - 128
    update_dims:
      - - 128
        - ${dataset.out_dim}
    edge_dims: ${dataset.edge_dim}
    final_activation: false

from egnn_pytorch import EGNN_Network
from torch import nn


class EGNN(nn.Module):
    """
    A wrapper around the original EGNN code, docstring autogenerated from the README of the package on Github. 

    Parameters
    ----------
    dim : int
        Input node feature dimension.

    edge_dim : int, default=0
        Edge feature dimension. Must be > 0 if edge features exist.

    m_dim : int, default=16
        Hidden message-passing dimension.

    fourier_features : int, default=0
        Number of Fourier features used to encode relative distances.
        Defaults to 0 (no Fourier features), as in the original paper.

    num_nearest_neighbors : int, default=0
        If > 0, caps the number of neighbors considered for message passing
        based on relative distance (take the closest K).

    dropout : float, default=0.0
        Dropout applied to feature updates.

    norm_feats : bool, default=False
        Whether to apply layer normalization to node features.

    norm_coors : bool, default=False
        Whether to normalize coordinates using the strategy
        from the SE(3) Transformer paper.

    update_feats : bool, default=True
        Whether the EGNN layer updates node features.

    update_coors : bool, default=True
        Whether the EGNN layer updates node coordinates.

    only_sparse_neighbors : bool, default=False
        If True, message passing is restricted to edges defined
        by the adjacency matrix (ignoring dense pairwise interactions).

    valid_radius : float, default=inf
        Maximum radius for valid message passing. Nodes farther apart than
        this value do not exchange messages.

    m_pool_method : {"sum", "mean"}, default="sum"
        Pooling method used to aggregate messages.

    soft_edges : bool, default=False
        Whether to apply an additional GLU to edge messages. This is a
        stabilization trick used in a later EGNN paper revision.

    coor_weights_clamp_value : float or None, default=None
        Optional clamp value applied to coordinate update magnitudes for
        additional training stability.
    """

    def __init__(
        self,
        *,
        depth,
        dim,
        num_tokens=None,
        num_edge_tokens=None,
        num_positions=None,
        edge_dim=0,
        num_adj_degrees=None,
        adj_dim=0,
        global_linear_attn_every=0,
        global_linear_attn_heads=8,
        global_linear_attn_dim_head=64,
        num_global_tokens=4,
        **kwargs
    ):
        super().__init__()

        self.model = EGNN_Network(
            depth=depth,
            dim=dim,
            num_tokens=num_tokens,
            num_edge_tokens=num_edge_tokens,
            num_positions=num_positions,
            edge_dim=edge_dim,
            num_adj_degrees=num_adj_degrees,
            adj_dim=adj_dim,
            global_linear_attn_every=global_linear_attn_every,
            global_linear_attn_heads=global_linear_attn_heads,
            global_linear_attn_dim_head=global_linear_attn_dim_head,
            num_global_tokens=num_global_tokens,
            **kwargs
        )

        self.transforms = []

    def forward(
        self,
        data
    ):
        node_out, _ = self.model(
            feats=data.x.permute(2, 0, 1),
            coors=data.pos.unsqueeze(0),
            adj_mat=data.adj_mat,
            edges=data.edge_attr,
        )
        return node_out
